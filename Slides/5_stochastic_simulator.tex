\documentclass[nopagenumber,9pt]{beamer}

\mode<presentation> {
  \usetheme[]{CambridgeUS}
  %\useoutertheme{shadow}
  \setbeamercovered{transparent}
  \usecolortheme{seahorse}
%\usecolortheme{sidebartab}
%  \usefonttheme{structurebold}
  \useinnertheme{default}
\useinnertheme{rounded}
}
\usepackage{nicefrac}
\RequirePackage{amsmath,amsfonts,amsthm}
\newtheorem{Prop}{Proposition}

\usepackage{float}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{url}
\usepackage[T1]{fontenc}
%\usepackage{multirow}
\usepackage{color}
\newcommand{\mb}[1]{\mathbf{#1}}
\usepackage{graphicx}
\graphicspath{{./figure/}}
\usepackage{multirow}

\definecolor{dgreen}{RGB}{139,172,100}

\hypersetup{
  colorlinks = true,
  linkcolor = black
}
\makeatletter
\let\@mycite\@cite
\def\@cite#1#2{{\hypersetup{linkcolor=dgreen}[{#1\if@tempswa , #2\fi}]}}
\makeatother

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xcolor,colortbl}
\usepackage{rotating}
\usepackage{multirow}

\usepackage{tikz}
\usepackage{ulem}
\newcommand{\argmax}[2]{% 
\smash{\mathop{{\rm argmax}}\limits_{#1}}\,#2} 
\usepackage{stmaryrd}
\newtheorem{proposition}{Proposition}
\newcommand{\I}{\mathbb{I}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\brho}{\boldsymbol{\rho}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\nf}{n_e}
\def\ee{{\mathbb E}}
\newcommand{\dd}{\mathrm{d}}

%%%macros
\newcommand{\yexpi}{y_{exp_i}}
\newcommand{\bxf}{\bx^e}
\newcommand{\byf}{\by^e}
\newcommand{\Df}{D^e}
\newcommand{\byc}{\by^c}

\newcommand{\ms}[1]{\boldsymbol{#1}}
\newcommand{\blue}{\textcolor{myblue}}
\definecolor{myblue}{RGB}{0,56,115}

\title[Stochastic Simulator]{Calibration of computer models}

%titre premiere page

\subtitle{Extension to Stochastic Simulator}

\author[P. Barbillon]{ Pierre \textsc{Barbillon}}
\bigskip

\date{Fall 2023}

\subject{Séminaire}



\AtBeginSection[] {
 \begin{frame}<beamer>
   \frametitle{Outline}
   \tableofcontents[currentsection]
  \end{frame}
}

\AtBeginSubsection[] {
\begin{frame}<beamer>
   \frametitle{Outline}
   \tableofcontents[currentsection,currentsubsection]
 \end{frame}
}


\begin{document}

\begin{frame}
\titlepage
%\includegraphics[scale=.12]{AgroParisTech_-_logo.PNG}
%\vspace{-1.5cm}
%\begin{flushright}
% \includegraphics[scale=.1]{Logotype-INRA-transparent.png}
% \end{flushright}
\vspace{-1cm}
\centering
\begin{tabular}{ccc}
 \includegraphics[scale=.08]{LogoUPSaclay.jpg}&
  \includegraphics[scale=1.3]{agrologo.png}&
   \includegraphics[scale=.1]{LogoINRAE.jpg}
\end{tabular}


\end{frame}

\section{Statistical Models}

\begin{frame}
 \frametitle{Stochasticity in Computer Experiments}
 
 
 The following is a basic model of a stochastic simulator experiment. If the code is run at a (vector) input $x$ producing a (scalar) output $y(x)$, this could be represented as:
\begin{equation}
y(x) = M(x) + v, \ v \sim N(0, \sigma_v^2(x)),
\label{eq:basic}
\end{equation}
where $M(x)$ is the expected value, $E[y(x)]$, of the output and $v$ is independent variability representing the randomness of the simulator. Its variance, $\sigma^2_v$, can depend on $x$, but constant variance is also possible. For deterministic simulators, $\sigma^2_v = 0$.


 Stochasticity as a mean of computation
 
 het or hom Stochastic
 
\end{frame}


\begin{frame}
 
\begin{equation}
y_F(x)  = y_S(x,u_C) + \delta_{\mathrm{MD}}(x)  + \epsilon,
\label{eq:obsmodel}
\end{equation} 
where $y_F(x)$ are real-world field observations at controllable (or measurable) inputs $x$, $y_S$ is the simulator with additional unknown, non-measurable, inputs $u_C$, $\epsilon$ is measurement error for the observations $y_F(x)$ (with variance $\sigma_{\epsilon}^2$), and $\delta_{\mathrm{MD}}(x)$ is an important term that accounts for the simulator not being a perfect representation of reality. $y_F$ ``observes'' reality with error $\epsilon$;  reality = $y_S+\delta_{\mathrm{MD}}$. 
 
 
 For stochastic problems, where reality is stochastic, the discrepancy term $\delta_{\mathrm{MD}}(x)$ cannot be assumed deterministic. Discrepancy in stochastic settings is an open research question, with little attention so far. The model for the discrepancy may need to be similar to the model for the simulator; for example, if modeling $y_S$ calls for a hetGP with a Mat\'ern 5/2 correlation function then it is possible that a hetGP is needed for the discrepancy as well (perhaps with a smoother squared exponential correlation). A full Bayesian analysis in such circumstances may be prohibitively expensive and the above procedure might need to be modified. 
\cite{sung2019calibration} use a hetGP for the discrepancy (but with a deterministic simulator), estimating parameters via maximum likelihood and following \cite{tuo2015efficient} to avoid confounding.

\end{frame}


\section{Heteroskedastic GP}

\begin{frame}
 \frametitle{Stochastic Kriging}
 % attention x pour toutes les entrées à changer ???
 
 Observation model: $$y_i^c=f(\bx_i)+\epsilon_i,\quad \text{with}\quad \epsilon_i\overset{ind}{\sim}\mathcal{N}(0,r(x_i))\,.$$
 In homoskedastic cases $r(x_i)=\tau^2$ which is called the nugget.
 
 
 
 Stochastic Kriging provides for a design with replications
 These make up the ``full-$N$'' dataset, $n$ of unique $x_i$-values in $X_N$ with  $n<<N$, $a_i$ replicates at unique locations 
 and 
 $$\bar{y}_i = \frac{1}{a_i} \sum_{j=1}^{a_i} y_i^{(j)} \quad \mbox{and} \quad
\hat{\sigma}^2_i = \frac{1}{a_i - 1} \sum_{j=1}^{a_i} (y_i^{(j)} - \bar{y}_i)^2.$$
 the following BLUP:
 
 \begin{align}
\mu^{\mathrm{SK}}_n(x) &= \nu k_n^\top(x) (\nu C_n + S_n)^{-1} \bar{Y}_n \notag \\
\sigma^{\mathrm{SK}}_n(x)^2 &= \nu K_\theta(x,x) - \nu^2 k_n^\top(x) (\nu C_n + S_n)^{-1} k_n(x),  \tag{10.1}
\end{align}

$k_n(x) = (K_\theta(x, \bar{x}_1), \dots, K_\theta(x, \bar{x}_n))^\top$
$S_n = [\hat{\sigma}_{1:n}^2] A_n^{-1} = \mathrm{Diag}(\hat{\sigma}_1^2/a_1, \dots, \hat{\sigma}_n^2/a_n),$ and $C_n = \{ K_\theta(\bar{x}_i, \bar{x}_j)\}_{1 \leq i, j \leq n}$

\cite{ankenman}
\end{frame}


\begin{frame}
 \frametitle{}
 as in \cite{Goldberg1998regression}
 Heteroskedastic GP modeling assumes
 $\log(r(x))\sim GP$
 with zero mean and variance 
 While full details are provided by \cite{Binois2018JCGS}, some specifics of the description above are worth noting.
With $\lambda(x)$ = $\sigma_v^2(x)/\sigma_Z^2$ and $\Lambda_n$  = $(\lambda(x_1), \dots, \lambda(x_n))$ for the $n$ distinct inputs, $\log\Lambda_n$ is taken to be the predictive \emph{mean} of a GP on latent (hidden) variables, $\Delta_n = (\delta_1, \dots, \delta_n)$. For ease of exposition assume the GP has 0-mean (a constant mean is actually the default setting in {\tt hetGP}) and take the covariance function for $\Delta_n$ to be $\sigma_g^2 (C_g + gR^{-1})$ where $g>0$, $R = \textrm{diag}(r_1, \dots, r_n)$, and $C_g$ is a correlation function with parameters $\theta_g$. Then $\log\Lambda_n = C_g(C_g + gR^{-1})^{-1}\Delta_n$. This latent $\Delta_n$ approach facilitates smooth estimates of $\Lambda_n$ and provides a fixed functional form for $\lambda(x)$, but does not incorporate the resulting uncertainty. Given $\Lambda_n$, the Woodbury identities  reduce the likelihood of $Y_N$, the output at \emph{all} inputs including replicates, to depend only on quantities of size $n$. Maximum likelihood estimates for the unknown parameters can then be computed at a cost of $O(n^3)$, as can derivatives further facilitating optimization for maximizing likelihood.
\end{frame}

\section{Calibration}


\subsection{KOH}

\begin{frame}
 \frametitle{Calibration of Stochastic Simulators}
  \begin{equation}
  \label{disc}
 \yexpi=f(\bx_i,\btheta^*)+\delta(\bx_i) +\epsilon(\bx_i)\,.
 \end{equation}
 $\delta(\cdot)$ models the difference between the simulator and the physical system:
 $$\delta(\bx)=\zeta(\bx)-f(\bx,\theta^*) \,.$$


Here $f$ is Stochastic but its link with reality is questionable.
Is reality $\ee(f)$ or $f$?

Depending on that, $\delta$ should be considered as deterministic or Stochastic and then modeled as a standard GP...

\end{frame}
\begin{frame}
 
 ocean example see https://github.com/Demiperimetre/Ocean
 
 
\end{frame}

\begin{frame}
 \frametitle{ref on HM}
 \cite{Andrianakis2015bayesian} contains a thorough description of HM whilst applying it to a complex epidemiology model of HIV.
\end{frame}

\subsection{ABC}

\begin{frame}
 \frametitle{basics}
 ABC is a general method for producing samples from $\pi(u_C | Y_F)$, the posterior distribution of unknowns $u_C$, given data $Y_F$. ABC does this by generating samples for the unknowns $u_C^{(s)}$ and the output $z^{(s)}$ from $\pi(Y_F | u_C)\pi(u_C)$, that is, from the likelihood of the data given the unknowns, multiplied by the prior probability of the unknowns. For computer models, generating samples from the likelihood is equivalent to running the simulator. Such samples are only accepted if $z^{(s)} = Y_F$. For continuous settings, where exact equality cannot occur, acceptance is instead made if $B(z^{(s)}, Y_F) < \tau$, where $B$ is a measure of distance and $\tau$ a level of tolerance. An approximated posterior distribution is then given by the collection of accepted $u_C^{(s)}$s. When there are multiple outputs (or there are other controllable inputs $x$, and so for any given $u_C^{(s)}$ there are effectively multiple outputs), $Y_F$ and $z^{(s)}$ can be replaced with informative summary statistics. Finding a single statistic sufficient for all outputs is challenging, and a poorly chosen one can invalidate results. 

The choice of the tolerance $\tau$ is also important. If $\tau$ is small then it may take a very long time to generate a single sample which satisfies the inequality. If $\tau$ is not small then the approximation to the posterior is less reliable. For calibration, $\tau$ can be interpreted as a bound on the observational error and model discrepancy, leading to a ``correct'' posterior rather than an approximation \cite{wilkinson2013approximate}. This is then similar to HM with the subjective choice of bounds. 

ABC can be done without the use of a surrogate, but this will require many runs of the simulator itself. Otherwise, very few accepted $u_C$ will be obtained, or an overly high value of $\tau$ will be required. In either case accuracy is compromised. Such computational barriers are alleviated by the use of a surrogate.


\end{frame}



\begin{frame}
fish example see \url{https://github.com/jhuang672/fish/blob/master/fish_fits.md}

\end{frame}


\bibliographystyle{apalike}
\bibliography{biblio}

\end{document}

